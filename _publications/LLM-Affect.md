---
title: "Can Large Language Models Capture Video Game Engagement?"
collection: publications
permalink: /publication/LLM-Affect
excerpt: ''
date: 2025-02-05
venue: 'Arxiv (Preprint). Submitted to IEEE Transactions on Affective Computing'
paperurl: ''
citation: 'Melhart, D., Barthet, M., & Yannakakis, G. N. (2025). Can Large Language Models Capture Video Game Engagement?. arXiv preprint arXiv:2502.04379.'
---

Can out-of-the-box pretrained Large Language Models (LLMs) detect human affect successfully when observing a video? To address this question, for the first time, we evaluate comprehensively the capacity of popular LLMs to annotate and successfully predict continuous affect annotations of videos when prompted by a sequence of text and video frames in a multimodal fashion. Particularly in this paper, we test LLMs' ability to correctly label changes of in-game engagement in 80 minutes of annotated videogame footage from 20 first-person shooter games of the GameVibe corpus. We run over 2,400 experiments to investigate the impact of LLM architecture, model size, input modality, prompting strategy, and ground truth processing method on engagement prediction. Our findings suggest that while LLMs rightfully claim human-like performance across multiple domains, they generally fall behind capturing continuous experience annotations provided by humans. We examine some of the underlying causes for the relatively poor overall performance, highlight the cases where LLMs exceed expectations, and draw a roadmap for the further exploration of automated emotion labelling via LLMs.


[Download paper here](http://matt-barthet.github.io/files/LLM-Affect.pdf)